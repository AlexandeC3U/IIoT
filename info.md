Custom IIoT Edge Platform – Technical Design & Specification
Introduction
Industrial Internet of Things (IIoT) applications demand an integrated platform that can collect data from diverse industrial devices, process and store time-series data (historian functionality), and present insights in real-time. This project outlines a custom IIoT edge platform (inspired by Litmus Edgedocs.litmus.iodocs.litmus.io), composed of containerized components orchestrated on-premises (Docker and Kubernetes) with optional cloud management. Key features include an MQTT broker (EMQX) for unified messaging, protocol conversion microservices for PLC and fieldbus integration (e.g. Siemens S7, OPC UA, Modbus to MQTT), a historian (time-series database for data logging), Node-RED for edge logic flows, Grafana for visualization, and an embedded management webapp (React/Vite based) for container orchestration and unified UI. This document provides a comprehensive technical specification – including architecture, component design, data flows, and future considerations – presented at a depth and rigor befitting a Ph.D.-level discussion. Diagrams and flowcharts (using Mermaid and other visualizations) illustrate the system’s design and operation, and connected sources are cited to ground key decisions in current best practices and research.
System Architecture Overview
At a high level, the platform follows a modular edge architecture where each function is delivered by a containerized service (Figure 1). These services communicate primarily via MQTT messages and REST APIs internally. The system is designed to run on-premises at the network edge (e.g. on an Industrial PC or gateway server), ensuring local operation even if cloud connectivity is lost. A cloud-based management console may supervise configurations and updates, but all critical data processing occurs on-premise for resilience and low latency. The solution leverages container orchestration (Docker with optional Kubernetes/K3s) to manage these services, allowing flexibility in deployment and scaling (e.g. spawning new protocol connector pods or analytic services on demand).
Figure 1: High-Level Architecture (Edge Platform with Modular Containers)
An overview of the proposed IIoT edge platform architecture. Industrial devices (PLCs, sensors) connect via protocol-specific connectors which publish data to the EMQX MQTT broker. The broker feeds other services: a Node-RED flow engine for processing/automation, a time-series database (historian) for data storage, and dashboards (Grafana) for visualization. A unified web frontend provides device management, container orchestration, and integrates component UIs (Node-RED, Grafana) via iframes. Optional cloud management can oversee edge nodes’ configurations.docs.litmus.io
Components and Data Flow
1.	Industrial Devices & Data Sources (OT Layer) – Machines, PLCs (e.g. Siemens S7), SCADA systems, sensors, etc., reside on the operational technology network. They expose data through various protocols (PROFINET/S7, OPC UA servers, Modbus RTU/TCP, etc.). The platform’s Device Connectivity services interface with these protocols to extract tag data (machine variables, sensor readings, etc.). For example, an OPC UA enabled PLC exposes a structured address space of tags which can be browsed and read; a Siemens S7 PLC might require reading specific DB addresses via an S7 driver or an OPC UA gateway. In later phases, automatic network discovery can be implemented to scan for known device IPs and protocols (e.g. using OPC UA discovery services or mDNS for certain devices), but initially device connections will be configured manually by the user (defining IP, protocol, and tag addresses).
2.	Protocol Conversion Microservices – For each supported industrial protocol, a containerized microservice (or Node-RED flow) is responsible for polling/subscribe to device data and publishing it to MQTT. These act as “southbound” connectors translating legacy or field protocols into modern MQTT messages (effectively creating a unified namespace of IoT datadocs.litmus.io). Possible implementations include:
o	OPC UA Connector: Uses an OPC UA client library (e.g. open62541 or Node-RED OPC UA nodes) to connect to an OPC UA server endpoint, browse for user-selected tags, and subscribe to their value changes. Data is encoded as MQTT topics (e.g. factory1/line2/OPCUA/<device>/<tag>) and payloads (JSON or binary) and published to the broker. Node-RED is well-suited here – it can serve as the data extraction layer for OPC UA, offering a low-code environment to integrate devices without extensive coding.
o	Siemens S7 Connector: Uses a Siemens S7 protocol library (such as Snap7 or Apache PLC4X) to read PLC memory areas. Since S7 is proprietary, often an OPC UA wrapper on the PLC or a gateway is used. Alternatively, a lightweight custom service can periodically read configured DB addresses via Snap7 and publish MQTT updates. (Siemens PLCs do not inherently advertise tag lists without a defined interface, so tag “discovery” may rely on PLC program metadata or be manual. In future, using OPC UA or unified namespace for Siemens controllers can simplify discovery.)
o	Modbus Connector: Uses a Modbus TCP/RTU library to read holding registers/coils from devices. A container polls configured register addresses at set intervals and publishes values to MQTT topics (e.g. factory1/line3/Modbus/<device>/<register>). Modbus being simple, tag discovery is essentially scanning address ranges, which might be left configurable.
o	Other Protocols: The architecture is extensible. Additional connectors for protocols like BACnet, EtherNet/IP (Allen-Bradley), PROFINET, etc., can be added as needed. The design favors modularity, where each protocol interface is isolated in its own container or Node-RED flow for maintainability. This mirrors the approach of commercial platforms like Litmus Edge, which offers a wide range of native drivers and DeviceHub modules to connect to PLCs and industrial assetsdocs.litmus.io. Once data flows from devices into the DeviceHub/connector services, it is normalized and forwarded into the central message brokerdocs.litmus.io.
3.	EMQX MQTT Broker – At the heart of the platform is an MQTT message broker (EMQX). MQTT is the de-facto lightweight publish/subscribe protocol for IoT, ideal for resource-constrained, high-latency, or unreliable networks. EMQX was chosen for its high performance and scalability – it is a distributed broker capable of handling massive numbers of connections and messages reliably. All data ingested by protocol connectors is published to EMQX under organized topic hierarchies (forming a Unified Namespace for all plant data). EMQX ensures decoupling between data producers (devices) and consumers (applications like Node-RED, historian, etc.), and supports features like persistent sessions and QoS levels for reliable delivery. The broker is configured to run in an isolated, secure mode (listening only within the edge network or on localhost for the containers). Role-Based Access Control (RBAC) and authentication can be enabled to ensure that only authorized services subscribe/publish to certain topics (mirroring Litmus Edge’s approach of using a secure, isolated broker for real-time datadocs.litmus.io). EMQX’s management API and dashboards will be available for monitoring throughput and topics if needed. In future, EMQX can also bridge messages to a cloud MQTT broker or IoT platform if cloud integration is required – by configuring it to forward specific topics to cloud endpoints (e.g., AWS IoT or Azure IoT Hub), enabling a hybrid edge-cloud data flow.
4.	Edge Data Processing & Automation (Node-RED) – To process and react to the streaming data, the platform integrates Node-RED, a popular open-source flow-based programming tool. Node-RED provides a visual flow editor in which users can wire together devices, data processing blocks, and APIs via drag-and-drop nodesinfluxdata.com. In this architecture, Node-RED plays multiple roles:
o	It can act as an ingestion pipeline (as described, connecting to devices like OPC UA clients). However, if dedicated connector microservices are used, Node-RED primarily subscribes to data from the MQTT broker as a consumer.
o	It implements edge logic and automation workflows. For example, a user can create a flow that subscribes to a temperature sensor topic, checks if the value exceeds a threshold, and then publishes a command to an actuator (via another MQTT topic or via a PLC write node) and also triggers an alert. This allows real-time local control decisions without cloud round-trip. The low-code interface significantly simplifies implementing such custom logic or analytics on the fly.
o	Node-RED can also serve to transform and forward data to the historian or external services. It has nodes for databases and web requests, so a flow might batch data and write to the time-series DB (if not using broker’s direct integration). It might also send data to an enterprise system or cloud API (e.g., REST calls or using an AWS/Azure node) if needed.
Node-RED’s flexibility and user-friendliness make it suitable for on-premises engineers to extend the system. In industrial contexts, Node-RED plus a time-series DB and Grafana provide a complete end-to-end IIoT data pipeline out of the box. Indeed, combining MQTT, Node-RED, and Grafana (often with InfluxDB) – known as the “MING” stack – is a proven approach for IIoT solutions. Our platform builds on this pattern, embedding Node-RED as the Flow Engine. The Node-RED instance runs in a Docker container, with its project files persisted (so flows are saved). We will pre-install useful Node-RED plugins, e.g., nodes for OPC UA, S7, Modbus, InfluxDB, etc., to support industrial integrations. The Node-RED UI (editor) will be accessible via the unified frontend (embedded via iframe) for authorized users to create or modify flows. For production safety, Node-RED can be run with User Authentication (it supports admin authentication) and possibly projects versioning enabled to track flow changes.
5.	Time-Series Data Historian – A core requirement is a historian component to store and allow querying of historical telemetry data. Traditional industrial data historians like OSI Pi have long provided reliable, compressed storage for time-stamped production data. However, they are often proprietary and not designed for easy integration with modern analytics or cloud systems. Instead, our platform will leverage a modern open-source time-series database (TSDB) for historian functionality. This database will run on the edge (containerized) to store all MQTT feed data of interest (e.g., sensor readings, device statuses over time). We consider two leading options:
o	InfluxDB – a popular TSDB purpose-built for time-series data. InfluxDB excels at handling high write loads and provides a SQL-like query language (InfluxQL or Flux) for analyticsinfluxdata.com. It has a small footprint and can run on edge devices. InfluxData’s latest open-source line (InfluxDB 3) even supports an embedded Apache Arrow/Parquet engine and can run at the edge or in the cloud, with the ability to replicate data upstream for central analysis. InfluxDB’s advantage is its widespread use in IoT, large community, and integration with Grafana is first-class (Grafana has a native InfluxDB data source). It also offers downsampling and retention policies to manage data volume over time. One consideration: the newer InfluxDB 3 (if used) is built on a columnar format and has in-memory components, which might need tuning for edge use; otherwise InfluxDB 1.x (the classic version) could be used for simplicity (though it’s older).
o	TimescaleDB – an industrial-grade time-series database built as an extension of PostgreSQL. Timescale provides full SQL support with time-series optimizations (compression, chunking, continuous aggregates). A notable example is IOTech’s Edge Historian service, which leverages TimescaleDB on PostgreSQL for edge deployments. TimescaleDB is highly reliable (inherits ACID properties from Postgres) and can handle high-volume writes while allowing complex queries (joins, etc.) if needed. It might be slightly heavier on resource usage than Influx, but offers the familiarity of SQL. Given our requirement for on-prem and potential cloud sync, Timescale could be a solid choice, enabling local storage with an option to replicate or batch-sync data to a central cloud PostgreSQL for aggregation across sites.
o	(Other TSDB alternatives): We also note emerging solutions like QuestDB (a high-performance columnar TSDB). In one example implementation, QuestDB was used at the edge to store industrial sensor data, chosen for its real-time ingestion and low-latency queries. QuestDB claims very high insert rates and uses SQL (with Postgres protocol compatibility). Another option is Apache IoTDB, designed for massive-scale IoT time-series data. And TDengine is an open-source TSDB specifically marketed for IoT/IIoT scenarios with built-in clustering and caching. Each has pros/cons in complexity and performance. For our design, we lean towards proven solutions like InfluxDB or TimescaleDB unless specific benchmarking dictates otherwise.
Historian Integration: Data will flow into the historian via one of two paths (or both for redundancy): (a) EMQX Rule Engine – EMQX supports a rule/bridge engine that can consume MQTT messages and directly write to databases or time-series stores. We can configure rules to insert incoming data into InfluxDB or Timescale (for instance, using HTTP write API for Influx or a JDBC/SQL connection for Timescale). This approach offloads data persistence to the broker level, ensuring every message is captured. (b) Node-RED Flow – Alternatively, Node-RED can subscribe to relevant MQTT topics and then use nodes or custom logic to batch and insert data into the TSDB. For example, a Node-RED flow could collect data points for 5 seconds and write a batch to InfluxDB (for efficiency). The Node-RED approach might be simpler to implement initially (leveraging existing contrib nodes like node-red-contrib-influxdb), while EMQX rules could be used for high-performance, parallel insertion. Both options ensure that the data is stored locally in chronological order with timestamps for trending, analysis, and compliance (e.g., storing 1 year of data for regulatory needs). The historian should implement data retention policies (deleting or down-sampling old data beyond a certain window if storage is limited) and support query APIs for dashboards and analytics.
In essence, this TSDB will function analogously to a traditional plant historian – capturing all telemetry with high reliability and allowing engineers to query historical trends – but built on open, modern technology. It provides the backbone for any AI/ML analytics in the future by retaining the necessary training data locally.
6.	Data Visualization (Grafana Dashboards) – To enable operators and engineers to monitor the system, the platform will include Grafana for visualization. Grafana is an open-source analytics and monitoring UI that connects to various data sources (including InfluxDB, TimescaleDB, etc.) and provides rich interactive dashboards. In this platform, Grafana will serve as the primary tool for creating charts and graphs of the time-series data (and possibly real-time values). We will deploy Grafana in a container and pre-configure it to use the historian database as a data source. Grafana’s role is purely read-only visualization (write actions can be handled via Node-RED or other means) – it excels at making complex data understandable through dashboards. For example, a production manager can view a dashboard of machine temperatures, pressures, and throughput over time; maintenance can see vibration trends and receive alerts, etc. Grafana supports alerting as well – it can query the data at intervals and trigger alerts (email, etc.) on conditions, which could complement Node-RED’s alerting. The web frontend will embed Grafana dashboards via iframes, likely using Grafana’s kiosk mode or direct dashboard links. Authentication to Grafana can be handled via Grafana’s internal user system or through a reverse proxy such that the unified platform login suffices (this may require some SSO integration or using Grafana API tokens in embed). Grafana’s flexible panels also allow embedding live MQTT data via plugins (there are MQTT data source plugins) for real-time updates, but the simpler approach is to read from the historian which is continuously updated. By integrating Grafana, we avoid “re-inventing” the UI for data visualization – it provides a professional, polished interface out-of-the-box, consistent with industry practice (Litmus Edge, for instance, touts ready analytics and KPIs for time-series datadocs.litmus.io, which in a custom solution we achieve by leveraging Grafana on top of our historian).
7.	Embedded Container Orchestration & Management – Unlike a monolithic application, our platform consists of multiple containers. To simplify deployment and management, we will use either Docker Compose or Kubernetes to coordinate these services. The design calls for an embedded container management application that abstracts the underlying container platform and lets users (or admins) manage the lifecycle of services via a web UI. This is similar in spirit to Litmus Edge’s “Applications -> Containers” module which allows starting/stopping containers and deploying new ones from a marketplacedocs.litmus.io. It’s also reminiscent of Portainer, a general container management UI that works with Docker and Kubernetes to handle containerized apps.
Orchestration Mode: We have two modes to consider:
o	Docker-Compose (single-node): All containers (EMQX, Node-RED, Grafana, TSDB, connectors, UI) can be defined in a Docker Compose file. This is straightforward for a single edge device and easy to setup. The management UI in this case would interact with the Docker Engine API (or docker CLI under the hood) to start/stop containers, monitor status, and launch new containers as needed (for example, launching an additional connector or analysis service).
o	Kubernetes (multi-node or advanced features): To enable scaling and more advanced scheduling, we can embed a lightweight K8s distribution such as K3s (Rancher’s lightweight Kubernetes) on the edge device. K3s is resource-friendly and simplifies Kubernetes for IoT/edge use cases. With K3s, each service runs as a pod, and the management UI can interface with the Kubernetes API (using client libraries) to manage deployments. Kubernetes brings features like auto-restart, self-healing, and easier horizontal scaling. For example, if a Node-RED pod crashes, Kubernetes will auto-redeploy it; if load increases, one could scale out Node-RED to multiple instances (with some load-balancing scheme). Given the project’s ambition, using Kubernetes aligns with future needs (clustering, cloud-managed fleet of edges). However, it adds complexity in initial setup compared to plain Docker. Both approaches can be supported: we design the management layer to be agnostic to Docker or K8s by abstracting the operations (start container/pod, stop, monitor logs, etc.). Under the hood, it can detect the environment (if K8s available) and use kubectl/API calls, otherwise fallback to Docker.
Container Management UI: The platform’s webapp will include a “Containers” or “Applications” page, where the user can see all running containers/pods, their status (running, stopped, etc.), resource usage, and manage them. This is essentially embedding functionality akin to Portainer or the Kubernetes Dashboard but in a simplified, integrated manner. For instance, Litmus Edge’s container pane shows each container color-coded by state (green=running, etc.) and even allows running custom Docker commandsdocs.litmus.iodocs.litmus.io. We will provide basic controls: Start/Stop/Restart a container, View Logs, and Deploy New Application. Deploying a new app could allow the user to upload a Docker image or select from a list of available images (similar to a marketplace catalog) and then the system will run it (possibly pulling from a registry). This mechanism enables extensibility – e.g., if later the user wants to add an AI microservice container, they could do so through this UI. The container management module will be secured (admin-only access) to prevent accidental disruption. Underneath, it will interface with container runtimes securely (likely via a backend API that has access to Docker socket or K8s credentials – the React frontend calls that backend).
Furthermore, auto-start and health monitoring will be implemented: configured essential containers should auto-start on system boot, and a watchdog can notify the UI if any container stops unexpectedly. “Auto spin new pods when needed” could refer to automatically deploying additional instances of certain services under high load (auto-scaling) or launching connectors when new devices are added. In a K8s setup, horizontal pod autoscalers could be used for certain stateless services. Initially, we might implement a simpler logic: e.g., if a user adds a new device of a type that needs a dedicated connector instance, the system programmatically starts a new container for that connector. This ensures the architecture can dynamically adapt to new requirements without manual intervention in the underlying infrastructure.
8.	Unified Web Frontend (React/Vite) – Sitting above all components is a unified web application that provides a cohesive user interface for the platform. This React-based frontend will be the primary way users interact with the system, and it ties together the UIs of Node-RED, Grafana, and the management functions into one place. The frontend likely communicates with a backend component (which could be a lightweight Node.js or Go server running as another container, providing REST APIs to interact with EMQX management, the TSDB for queries (if needed), and the Docker/Kubernetes control). The UI will have a clean, modern design (leveraging Vite for fast development and bundling) and will be accessible via a web browser on the local network (and optionally through cloud if portaled). Key aspects of the frontend:
o	Dashboard Home: A summary view showing system status – e.g., broker status (connected devices, message rates), Node-RED status, DB status (disk usage, recent data rate), perhaps a couple of key Grafana charts for an overview.
o	Devices/Tags Management: An interface to configure connected devices and their data tags. For example, a form to add a new PLC: choose protocol (OPC UA, S7, Modbus, etc.), provide connection details (IP, credentials, etc.), and then either manually input tag addresses or browse available tags (if protocol supports browsing, like OPC UA). The UI might leverage the connector services for browsing (e.g., an OPC UA connector can expose an API to browse the server and return a tag tree). The selected tags can then be mapped to MQTT topics in the configuration. This module essentially orchestrates the DeviceHub functionality (similar to how Litmus DeviceHub allows configuring drivers and data pointsdocs.litmus.io). Once configured, the backend could deploy or configure the corresponding connector service to start data acquisition.
o	Data Flows (Node-RED): We will embed Node-RED’s editor UI within our React app (likely via an <iframe> that loads the Node-RED editor, which normally runs at e.g. http://localhost:1880). This allows users to create automation flows. We may customize Node-RED’s appearance or at least ensure single sign-on so that the user doesn’t need to log in separately. The frontend can also provide a library of example flows or templates (for common tasks) to accelerate usage.
o	Analytics Dashboards: Similarly, Grafana dashboards will be embedded via iframe. We can have a menu for “Dashboards” which, when selected, shows Grafana’s UI embedded and possibly filtered to a particular dashboard. Alternatively, for more seamless integration, we could use Grafana’s HTTP API to fetch panel images or data and render certain charts natively in React. However, given the complexity of recreating Grafana’s functionality, embedding is the straightforward route. Each dashboard can also be configured in Grafana itself; our UI just provides navigation.
o	Container Management: A section (for admins) showing the list of containers/pods as described. The UI should allow selecting a container to view details (uptime, resource usage if possible via APIs, logs stream) and buttons to control it. We might reuse components from existing projects (for example, Portainer’s API or open-source components) to implement this faster. We should also provide a way to add new containers (by image name + optional settings) – for advanced users or in later versions, a catalog of certified applications (much like a private marketplace). This ties into potential future integration of an app marketplace concept, where third-party or custom apps (packaged as Docker images) can be deployed at the edge to extend functionality (e.g., an AI inference service, or a custom data processor).
o	Settings & Security: The frontend will also cover system settings like user management (creating users with roles – at least admin vs viewer roles, similar to Litmus which supports RBACdocs.litmus.io), network settings, backups, etc. Security-wise, the webapp itself will require authentication (username/password login). Communication to the browser will be over HTTPS (with a self-signed or provided certificate)docs.litmus.io. The various component UIs (Node-RED, Grafana) can be set to either trust the main auth or have their own; a unified SSO is ideal but could be complex, so initially we may simply prompt for Node-RED’s password and Grafana’s API key when embedding, unless an integration is achieved. The platform must be “offline-first” given on-premise deployment (able to run fully without internet)docs.litmus.io, and it should support remote access only through secure methods (VPN or cloud proxy).
o	Production Frontend Look & Feel: Since the user emphasized a “very good looking and functional webapp”, significant attention will be paid to UI/UX design. A modern React component library (like Material-UI or Ant Design) can be used for consistency. The design will use intuitive dashboards, modals for configuration steps, and possibly wizards (e.g., a wizard to connect a new device from discovery to data flowing into a chart). The layout should integrate the iframed apps in a seamless way – for instance, hiding their standalone menus if possible, to make it feel like one application. We will also include company branding or custom styling as needed, since this is a custom platform.
Data Flow and Use Case Scenarios
To better illustrate how the system works, we describe a few key flow scenarios and how data/information moves through the platform:
1. Industrial Device Data Ingestion and Monitoring Flow
This flow covers collecting data from a PLC and visualizing it on a dashboard with historical trends:
Step 1: Configure Device & Tags – The user (engineer) accesses the “Devices” section in the UI and adds a new device entry. For example, a Siemens S7-1500 PLC on IP 192.168.1.50. The user selects the protocol “S7” and enters connection parameters (rack/slot or TCP port, and maybe credentials if needed). They then specify which data points to acquire – e.g., DB1.DBD0 (a temperature value) and DB1.DBD4 (a pressure). Since Siemens S7 doesn’t allow browsing tags easily, the user must know these addresses from the PLC program. (If it were an OPC UA device, a browse dialog could list available nodes to select.) The UI sends this config to the backend, which in turn either configures an existing S7 connector service or spins up a new connector container dedicated to this PLC (depending on architecture – one connector could handle multiple PLCs or one-per-device isolation for reliability). Let’s say each connector container is a Node.js or Python service using Snap7 library. The container starts and establishes a connection to the PLC.
Step 2: Data Published to MQTT – The S7 connector service begins reading the configured addresses in a loop (e.g., every 1 second). Each read value is published as an MQTT message to the EMQX broker, under a topic like site1/plantA/PLC1/DB1/DBD0 for the temperature. EMQX receives these messages and makes them available to any subscribed client. The connector may also publish a birth message on startup (to indicate the device is online, which could be used for system health monitoring).
Step 3: Broker Routes to Subscribers – The Node-RED flow engine is already subscribed (wildcard subscription) to site1/plantA/# or specifically to the PLC’s topics if configured so. It receives the temperature and pressure values in real-time. By default, Node-RED might just pass them along, but imagine we have a simple processing flow: Node-RED subscribes to the temperature topic, and if the value exceeds 80°C, it publishes a message to an “alerts” topic and also triggers an output (for example, turning on a cooling fan via writing to another PLC address or sending an MQTT command that another service acts on). This demonstrates local control. Meanwhile, EMQX also forwards the same data to the historian: we have an EMQX rule that whenever a message on site1/plantA/PLC1/# arrives, it writes the timestamp and value into the TSDB (InfluxDB). Alternatively, Node-RED could have an InfluxDB out-node wired after a MQTT-in node to store the values. Either way, the data point is appended to the time-series database with a timestamp.
Step 4: Historian Stores Data – InfluxDB (for example) stores the data in a measurement (e.g., plantA_PLC1) with fields temperature and pressure and tags for device, location, etc., or TimescaleDB stores it in a table. Over time, this builds a historical record. The system might also compute aggregate values – e.g., Node-RED could compute a 1-minute average and store that separately if needed for long-term trends (to reduce storage for high-frequency data). The historian is tuned to handle the insert rate of all devices – in a large deployment possibly thousands of points per second – using the optimizations of the chosen TSDB (compression, batch writes, etc.).
Step 5: Visualization and User Interaction – The engineer now goes to the “Dashboards” section of the web UI. We have a pre-built Grafana dashboard for PLC1 that shows a time-series graph of temperature and pressure. This Grafana dashboard queries the historian for the last 8 hours of data and updates in real-time (Grafana can poll every few seconds). The engineer sees the trend and can zoom in, set custom time ranges, etc., through Grafana’s controls. Suppose the temperature crosses a threshold; Grafana could show it in red and an alert icon. Additionally, because Node-RED published an alert message to an alerts topic, the UI might have an alerts panel (or Grafana panel) that shows current active alerts (Node-RED can also push alerts into a database or directly to Grafana’s alert API). The engineer acknowledges the alert and takes action (maybe shuts down a machine).
Throughout this flow, the data never left the on-premise environment – all decisions and storage were local, ensuring low latency and high reliability even if internet is down. Optionally, if cloud connectivity is configured, EMQX could also bridge the data to a cloud IoT platform for centralized monitoring across multiple sites, but that is outside the core on-prem scenario here.
The following sequence diagram summarizes this flow:
sequenceDiagram
    participant PLC as Siemens S7 PLC
    participant S7Conn as S7 Connector Service (Container)
    participant MQTT as EMQX Broker
    participant NR as Node-RED Flows
    participant TSDB as Historian (TSDB)
    participant Grafana as Grafana Dashboard UI
    participant User as Engineer User
    Note over PLC,S7Conn: 1. Connector is configured with PLC IP & tags.
    S7Conn->>PLC: Read DB1.DBD0, DB1.DBD4 (poll 1s)
    PLC-->>S7Conn: Returns values (temp, pressure)
    S7Conn-->>MQTT: Publish MQTT topics<br/> "site1/plantA/PLC1/DB1/DBD0" = temp<br/>"site1/plantA/PLC1/DB1/DBD4" = pressure
    MQTT-->>NR: Broker forwards messages (subscribed topics)
    MQTT-->>TSDB: Rule engine writes data points to historian
    NR->>NR: Flow logic checks temp value
    alt High Temp Detected
        NR-->>MQTT: Publish "site1/alerts/high_temp" (e.g., JSON payload)
    end
    TSDB-->>Grafana: Grafana queries recent data (SQL/InfluxQL query)
    User->>Grafana: Views dashboard (graphs of temp & pressure)
    Grafana-->>User: Renders time-series chart (auto-refresh)
    Note over User,Grafana: 5. User sees real-time and historical trends on Grafana
Figure 2: Data flow from device to dashboard. In this sequence, (1) the connector reads from the PLC, (2) publishes to MQTT, (3) Node-RED processes the data and triggers alerts, (4) data is stored in the historian, and (5) Grafana visualizes it for the user.
2. Edge Deployment and Container Management Flow
This scenario covers how the system and new applications are deployed/managed in production:
Step 1: System Deployment (Edge On-Premise) – The platform can be packaged as a set of Docker containers (with an orchestration script). For example, using Docker Compose, a YAML file defines all core services (EMQX, Node-RED, TSDB, Grafana, UI, etc.) and their interconnections. The edge device (which could be an industrial PC or a server in a factory) runs a Linux OS with Docker (and optionally K3s). The user (or integrator) installs the platform by running the compose or helm chart, bringing up all containers. Each container registers with the orchestrator (Docker or K8s). The management UI container starts and could check all others’ status via an API call (e.g., Docker Engine API). The user accesses the UI through a browser via the device’s IP (e.g., https://edgebox.local). They log in with admin credentials set during setup.
Step 2: Container Monitoring – On the UI’s Containers page, the user sees a list of services: EMQX – running, Node-RED – running, etc. Each has an indicator (green dot for running, etc., as in Litmus Edge’s UIdocs.litmus.io). The UI fetches this info by calling an endpoint on the backend which queries the Docker daemon or kubectl for pod statuses. If one of the services had crashed, it would show as stopped (and K8s would likely restart it automatically, but say it’s a custom app that exited). The user can click “View Logs” which streams the last N lines of the container’s stdout (the backend will fetch via Docker API). They can also adjust resource limits or restart policy if this is allowed (for advanced admin).
Step 3: Deploying a New Application Container – Suppose the user wants to integrate a Node-RED Companion container for heavy image processing at the edge, or an AI model inference service. They have a Docker image for it (e.g., hosted on a registry). In the UI, they click “Deploy New Container”. A form asks for the image name (and tag), optional environment variables, and resource limits. The user enters myrepo/edge-ai:1.0 and sets it to mount a certain volume or use an MQTT connection string. Upon submission, the backend either executes docker run ... or creates a Kubernetes Deployment for that image. The new container is pulled and started. The UI refreshes, and now “edge-ai” appears in the list with status running (if successful). The user can then manage it like other services. This demonstrates how the platform is extensible by the end user, not just fixed to built-in components.
Step 4: Automated Pod Scaling (Optional) – The system can automatically scale certain components if needed. For example, if Node-RED is heavily utilized (perhaps many flows causing CPU load), the orchestrator might launch another Node-RED instance to distribute load. In Kubernetes, this could be achieved with an Horizontal Pod Autoscaler (HPA) based on CPU or memory metrics. Our UI would then list “Node-RED (instance 1), Node-RED (instance 2)” or treat it as a scaled set. The user might not manually trigger this, but it’s a design consideration for robustness. Similarly, if an OPC UA connector is overloaded reading too many tags, one could deploy a second instance splitting the load. Auto-spinning pods would require predefined rules (or manual triggers from the UI like “Scale out Node-RED to 2 instances”). Initially, the platform might not implement complex auto-scale, but it’s designed to allow it, especially under Kubernetes.
Step 5: Software Updates – When an update to a container or configuration is needed, the cloud management interface (if in use) could push it. For example, a cloud “Edge Manager” might instruct the edge to pull a new image version. Alternatively, the user can update images via the UI by specifying a new tag and restarting the container. We can incorporate an “OTA update” mechanism similar to Litmus’s approach (their Edge supports over-the-air updates with verificationdocs.litmus.io). This ensures the platform can be maintained with minimal downtime.
3. Cloud Configuration and Multi-Edge Coordination (Future Scenario)
Although the initial deployment focuses on a single on-premise instance, a longer-term vision involves a cloud-based management system that can oversee multiple edge nodes and unify their data. In this scenario:
•	The Edge Manager (Cloud) is a web service (possibly a multi-tenant cloud app) that the on-premise edge nodes register with. Each edge node has an agent (or uses the existing components) to communicate status and receive config from cloud. Litmus Edge, for example, has an Edge Manager which consolidates data from many edge devices and allows remote configurationdocs.litmus.io.
•	The Cloud Configuration could allow a user to log in and see all their sites/edges, apply configurations (like deploying a new flow or connector to many edges at once), and gather data for higher-level analytics.
•	Each edge would still run autonomously (data primarily processed locally), but periodically can sync summaries or the full data to cloud storage (for enterprise-wide analysis). For instance, the historian might replicate key metrics to a cloud data lake or an IoT cloud service. In our design, EMQX could bridge messages northbound to a cloud MQTT broker, or a separate service could batch-upload time-series data to cloud databases.
•	Security is critical: edges would outbound connect to cloud (to avoid opening inbound ports on factory network). This could be via MQTT TLS connections or HTTPS to a REST API. All traffic would be encrypted and authenticated (certificates, tokens, etc.)docs.litmus.io.
•	The cloud interface can also store configurations (desired state for each edge) and edges check in to apply them (similar to an IoT device twin concept). For example, to add a new device on Edge #5, an admin could use the cloud UI to configure it, and the edge agent receives that and sets up the connector locally.
This multi-edge setup is beyond the immediate scope but is mentioned to ensure the architecture is cloud-ready. By using containerization and Kubernetes, our platform on each edge can be seen as a unit that could be managed by a higher-level orchestrator across sites. Ensuring components are decoupled and using standard interfaces (MQTT, REST, etc.) facilitates this future expansion.
Best Practices and Key Technical Decisions
This section details some rationale behind the chosen components and approaches, referencing best practices and comparable systems:
•	MQTT Unified Namespace: Using a centralized MQTT broker internally implements the concept of a Unified Namespace (UNS), which is a best practice for IIoT data architecture. All data is published into a structured namespace where any application can subscribe. This decouples sources and sinks of data, enabling easy integration of new services (just subscribe to the relevant topic) and external systems if needed. Litmus Edge’s adoption of an internal broker for real-time data is an example of this designdocs.litmus.io.
•	Protocol Drivers vs Flow-Based Integration: We considered whether to use Node-RED for all device interfacing versus dedicated microservices. Node-RED’s strength is ease of integration, but for very high-frequency data or proprietary protocols, a compiled service might be more efficient. A hybrid approach is ideal: start with Node-RED and existing open nodes for OPC UA, Modbus, etc., then gradually replace with optimized services as needed. The platform could even incorporate an industrial connectivity gateway like EMQX Neuron (which is specifically made for converting protocols like OPC UA, Modbus, Siemens S7, etc. to MQTT). EMQX Neuron is a commercial component, but an open-source analog is EdgeX Foundry’s Device Services (EdgeX provides connectors for many protocols, which feed into an internal message bus). We can take inspiration from those projects for structuring our connectors.
•	Historian Choice: The selection of a time-series database is crucial for performance. Traditional historians are robust for OT needs (reliability, compression, one-stop solution), but they lack openness. Our use of an open TSDB not only meets historian requirements but also ensures scalability and integration with analytics/AI. We will, however, ensure the chosen TSDB is configured for high retention and compression (to store possibly billions of points). TimescaleDB’s built-in compression or InfluxDB’s TSM engine (or Parquet in v3) will be leveraged. Additionally, we plan periodic export or archiving of old data (maybe to cloud or external storage) to keep the edge database size manageable long-term.
•	Grafana Integration: Grafana is a proven tool for industrial dashboards; many practitioners use the MING (MQTT+Influx+NodeRED+Grafana) stack in manufacturing pilots. By using Grafana, we avoid developing a custom charting UI from scratch, saving time and gaining a full feature set (zoom, alerts, user-defined dashboards). One challenge is user management overlap – we will likely have Grafana use an internal user but hide it behind our UI. It might be acceptable initially that the user logs into Grafana separately (embedded iframe can still show login page), though a smoother integration can be done by using a reverse proxy and auto login. These are implementation details to refine, but not blockers for design.
•	React Frontend and UX: The choice of React with Vite ensures a snappy, modular front-end. React’s component model is well-suited to building dynamic dashboards and forms (for device config, etc.). We’ll emphasize responsive design (so it could even be used on a tablet on the factory floor). Complex operations, like browsing an OPC UA server for tags, will be implemented with appropriate UI elements (tree views, search filters) to handle potentially large tag lists. We will also incorporate visualizations in the documentation and UI for flows – e.g., using Mermaid for designing flows or diagrams (as done in this spec) which can also be conceptually shown to end users for system understanding.
•	Security Considerations: Since this platform touches OT (Operational Technology) networks, security is paramount. We will isolate containers by network where possible (e.g., connectors might have access to the OT subnet, but others do not). The MQTT broker requires username/password or client certificates for all clients to prevent unauthorized publishingdocs.litmus.io. The web UI uses HTTPS and has its own auth (which should be integrated with enterprise auth in future, e.g., LDAP/AD as Litmus allowsdocs.litmus.io). Internally, sensitive data (like credentials for PLCs) will be stored encrypted in configuration. Role-based access will be enforced in the UI (e.g., an “Operator” role might only view dashboards, but “Engineer” role can edit flows and configs, and “Admin” can manage containers). These measures align with industrial security best practices and what platforms like Litmus advertise (RBAC, secure communications, etc.)docs.litmus.iodocs.litmus.io.
•	Performance and Reliability: Each component choice was also evaluated for performance. EMQX is capable of large throughput (million messages per second range on decent hardware, according to EMQX benchmarks). Node-RED, being single-threaded (Node.js), might become a bottleneck if overused; hence critical logic might be put in other services or scaled out. The TSDB will likely be the heaviest component in terms of disk I/O and memory; appropriate hardware (fast SSDs for DB) and tuning (write batch size, retention policy to drop high-res data after X days) will be planned. Kubernetes (if used) will provide a self-healing mechanism for services (always keep desired number of pods running). Backups for the TSDB (periodic snapshot or replication) need to be scheduled to avoid data loss in event of hardware failure. The system as a whole should be tested under load – e.g., simulate 1000 tags at 1s interval – to ensure it meets the performance needs. If needed, partitioning across multiple edge boxes (each handling subset of devices) can be done, but presumably one instance should handle a small factory’s data.
•	Comparative Note on Litmus Edge: Our custom platform essentially recreates many features of Litmus Edge – connectivity, data processing, containerization, analytics. Litmus Edge’s strength is in providing out-of-the-box drivers and a polished UIdocs.litmus.io; our approach achieves similar functionality with open-source components. For example, Litmus’s Flows module is effectively Node-RED-like (allowing flow-based programming), and Litmus’s Analytics/KPI can be achieved with Grafana on our side. The Embedded container management we propose is directly analogous to Litmus’s Applications > Containers UIdocs.litmus.io, which confirms the validity of that design. Thus, we are confident that this architecture is aligned with industry-leading solutions, but is flexible for customization and future expansion (especially integration of AI and cloud).
Future Extensions and Versions
While the initial scope is extensive, there are already hints of future requirements such as AI integration and advanced discovery. Here we outline how the design can accommodate those in later versions:
•	Advanced Data Analytics & AI/ML: In later versions, we can integrate AI capabilities either by adding specialized containers or by leveraging built-in features of our TSDB. For example, InfluxDB 3 offers an embedded Python engine for anomaly detection and real-time pipelines directly inside the database. We could utilize that to run Python ML models on the time-series data in situ. Alternatively, we deploy a container with an ML model (say a TensorFlow Lite model) that subscribes to MQTT topics and publishes results (like anomaly scores or predictions). Because our architecture is message-centric, adding such an AI service is straightforward: it’s just another MQTT client processing data. The Node-RED flows can orchestrate between raw data and the AI service (e.g., send a batch of data to an AI model container and get back a prediction). Use cases might include predictive maintenance (model predicts machine failure from vibration patterns) or quality inspection (an image classification container could analyze images if we feed it). The platform’s container management UI would facilitate deploying new AI services as needed. Grafana could then visualize AI outputs or alarms. We must ensure the edge hardware is sufficient for AI tasks (possibly with a GPU). Also, training of models would typically occur in the cloud or offline, but the inference runs at edge for real-time response. A future cloud integration can manage deploying updated ML models to all edge nodes (a common need in scaling AI in IIoT).
•	Network/Device Discovery: As mentioned, automatic network discovery of devices is a desirable feature to simplify onboarding. In a future release, we could implement a discovery service that periodically scans the local network for known device signatures:
o	For IP-based protocols (Modbus TCP, Ethernet/IP, OPC UA), this could mean scanning IP ranges on certain ports (e.g., port 502 for Modbus, 4840 for OPC UA) to find responsive devices. If found, attempt to identify them (OPC UA servers can list their endpoint info, which often includes the server name/vendor).
o	Using protocols like SNMP or LLDP on networks to identify devices and their types.
o	mDNS/ZeroConf for any devices that advertise (less common in industrial but some IoT sensors do).
o	A specialized approach: Siemens S7 Discovery (S7 has a mechanism via sending UDP packets on port 48899 or similar to find PLCs), or using PROFINET DCP broadcast to find devices on a subnet. These are more advanced but could be integrated with existing libraries.
o	After discovery, the system could list “Unconfigured devices found:” with some identifiers, and allow one-click adoption (creating a device entry for them).
This feature must be carefully implemented to avoid network load and adhere to security (some OT networks won’t like random scanning). It can be off by default or only run on user initiation.
•	Edge-to-Cloud Data Sync: We touched on cloud, but specifically for historian data, an extension could be to implement a cloud historian integration. E.g., periodically sync edge TSDB data to a central cloud TSDB or data lake. This could use native replication (Timescale can replicate via PostgreSQL replication; InfluxDB might ship segments or use their cloud relay). Or simply a batch export: Node-RED or a cron job could daily push yesterday’s data to cloud storage (CSV or Parquet files). This ensures long-term aggregation across sites for corporate analytics while keeping edge storage bounded. Another approach is leveraging cloud IoT services: e.g., AWS IoT SiteWise is an IIoT data store that could ingest from our edge. In fact, an AWS guide describes using Litmus Edge to feed SiteWise – our system could similarly publish to SiteWise or Azure IoT for cloud-side dashboarding.
•	Digital Twin and MES/ERP Integration: The data model can be expanded to include asset models (digital twins) where raw tag data is contextualized as meaningful assets (pump, motor, etc. with attributes). Litmus has a Digital Twins module for this. We could introduce an intermediate layer in software (or simply in how topics are structured, e.g., use MQTT topics to represent assets). This would help when integrating with higher-level systems like MES (Manufacturing Execution Systems) or ERP – they prefer contextualized data. Through Node-RED or additional microservices, we could implement connectors to those enterprise systems (like sending production counts to an ERP via an API, etc.).
•	User Journey and Onboarding: Future improvements also include making the system easier to set up and use. Possibly adding guided tutorials in the UI, or a simulation mode with fake device data for training. Since the platform is quite feature-rich, providing clear documentation (perhaps integrated into the UI via help sections) will be important.
Conclusion
In summary, this technical specification has delineated a comprehensive IIoT edge platform that unifies data connectivity, processing, storage, and visualization in a modular, containerized architecture. By drawing on proven open-source components – EMQX for MQTT messaging, Node-RED for flow-based logic, InfluxDB/TimescaleDB for time-series storage, and Grafana for visualization – the system achieves functionality on par with leading industrial platforms like Litmus Edge, but with the flexibility of customization and open standards. We detailed how each component interacts, how data flows from factory sensors to actionable insights on dashboards, and how the platform can be operated and extended. The use of Docker/Kubernetes not only encapsulates each service for reliability and scalability, but also enables a powerful embedded container management UI, empowering users to deploy and control additional services (a key to future-proofing the solution).
Crucially, the design emphasizes on-premise operation (for resilience and security in OT environments) with optional cloud configuration and integration, aligning with the trend of hybrid IIoT deployments. We also considered the historian implementation in depth – opting for modern TSDBs that provide the performance and openness required for advanced analytics, addressing the limitations of legacy historians in an Industry 4.0 context. Security best practices are woven throughout the design, from encrypted communications to role-based access, acknowledging the critical nature of industrial systems.
This platform is inherently ambitious and extensible. Its architecture can accommodate future additions like AI/ML analytics, broader protocol support, and multi-site management without fundamental changes. Each module can evolve: for instance, the Node-RED engine could be supplemented or replaced by a more specialized real-time rules engine if needed, or the MQTT broker can be clustered for redundancy. The component-based design (with well-defined interfaces like MQTT and REST) ensures that improvements or swaps (e.g., using a different visualization tool or adding a second broker) can be done with minimal disruption.
By fully understanding the needs and flows involved in industrial data management, we have crafted this philosophy of design that is both theoretically sound and practically implementable. The use of rich visualizations and diagrams herein (architecture diagrams, sequence flows) aids in communicating the complex interactions clearly. These should serve as references during development and deployment, ensuring all stakeholders (developers, engineers, and decision-makers) have a shared understanding of how the system works.
Ultimately, this document serves as a full technical specification at a high level of detail, guiding the development of the platform. Each cited reference reinforces that our design choices are grounded in current industry practices and research, from the concept of an IIoT MING stack to the capabilities of Litmus Edge that we mirrordocs.litmus.io. The result will be a robust, scalable, and user-friendly IIoT edge solution that can drive smart manufacturing and industrial digitalization efforts effectively into the future.
Sources: The design decisions and context are informed by industry documentation and examples, including Litmus Edge technical docsdocs.litmus.iodocs.litmus.io, open-source IoT framework guidesinfluxdata.com, and expert blogs on implementing MQTT-based IIoT pipelines, as well as considerations on historians from InfluxData and IOTech’s Edge insights, among others. These references underline the feasibility and validity of the proposed architecture within the current state of IIoT technology.
Technical Specification: Industrial IoT Edge Platform (Docker/Kubernetes-Based)
Overview and Objectives
This project aims to design an Industrial IoT Edge Platform that integrates multiple components for data acquisition, processing, storage, and visualization. The system is Docker container-based, potentially orchestrated with Kubernetes, and draws inspiration from Litmus Edgelitmus.io. The platform will run on-premise at the factory/edge level, while also being configurable via cloud for centralized management. Key objectives and features include:
•	EMQX MQTT Broker for reliable, scalable messaging between devices and applications (capable of handling millions of messages and devices)grafana.com.
•	Protocol Conversion & Device Connectivity: Support industrial protocols (Siemens S7, OPC UA, Modbus, etc.) and convert their data to MQTT, enabling a unified communication layer.
•	Automatic Tag Discovery (in future phases): Ability to discover data tags/points from connected devices or PLCs, simplifying configuration (network scanning will be a later-stage feature).
•	Data Processing Flows: Integration with Node-RED for low-code orchestration of data flows, transformations, and logic (Node-RED provides an intuitive flow-based interface for complex IoT workflowsemqx.comflowfuse.com).
•	Historian (Time-Series Data Storage): A robust time-series database to act as a data historian for logging sensor data efficiently and enabling queries for analyticsemqx.com. The approach should ensure high write/read performance and retention management for IoT data.
•	Data Visualization: Grafana dashboards for real-time visualization of sensor data and trendsemqx.com, allowing users to monitor KPIs and historical trends through a web interface.
•	Embedded Container Management: A unified web application (React-based) that allows managing the containers and microservices (similar to Portainer or Litmus Edge's Apps) from an easy UI, including deploying new services or auto-spinning new pods when needed.
•	Integrated Frontend: A polished React (Vite) web app that brings all components together, potentially embedding UIs (Node-RED editor, Grafana dashboards, etc.) via iframes for a seamless user experience.
•	Edge-Cloud Hybrid Management: The system will primarily run on-premise (at the edge) for reliability and low latency, but it should be configurable and controllable from a cloud portal. This allows remote updates, centralized configuration, and fleet management across multiple edge nodes.
•	Future Extensions: Design consideration for future features like network device discovery (auto-detecting devices/tags on the network) and AI/ML integration (e.g. predictive analytics, anomaly detection at the edge) in later versions.
By fulfilling these objectives, the platform will function as a comprehensive Industrial IoT solution that collects data from diverse industrial equipment, normalizes and transports that data via MQTT, stores it in a historian, and provides tools for automation logic and visualization. All of this will be encapsulated in a modular, containerized system that can be easily managed and scaled.
High-Level System Architecture
The system follows a modular microservices architecture with each major function running in a container. The containers are orchestrated using Docker and possibly Kubernetes (e.g. a lightweight distribution like K3s for edge deployments). The architecture consists of the following main layers and components:
flowchart LR
 subgraph Edge_Node["On-Premise Edge Node"]
    direction LR
    %% Device connectivity layer
    Device1([PLC - Siemens S7]) -- S7 protocol --> S7Connector[[S7 Connector Service]]
    Device2([OPC UA Device]) -- OPC UA --> OPCConnector[[OPC UA Connector Service]]
    Device3([Modbus Device]) -- Modbus --> ModbusConnector[[Modbus Connector Service]]
    %% MQTT broker
    S7Connector --> MQTTBroker((**EMQX MQTT Broker**))
    OPCConnector --> MQTTBroker
    ModbusConnector --> MQTTBroker
    %% Data processing layer
    subgraph Processing["Data Processing & Logic"]
      NodeRED[[Node-RED Flows]]
    end
    MQTTBroker ~~~ NodeRED %% using MQTT pub/sub (bidirectional)
    NodeRED --> TSDB[(Time-Series Historian DB)]
    %% Visualization
    TSDB --> GrafanaUI[Grafana Dashboards]
 end
 %% Management layer
 subgraph Management_Portal["Unified Management Portal (React Web App)"]
    UIReact[[React Frontend]]
    UIReact -. iframe .-> GrafanaUI
    UIReact -. iframe .-> NodeRedUI[Node-RED Editor UI]
    UIReact -. iframe .-> ContainerUI[Container Mgmt UI]
    CloudMgr[(Cloud Config Service)]
    CloudMgr --> UIReact
    CloudMgr -->|OTA updates| Edge_Node
    UIReact -->|Deploy/Control| Edge_Node
 end
Figure: Overall architecture of the IoT Edge Platform. Industrial devices (left) are connected via protocol-specific connectors, which publish data to the EMQX MQTT broker. Node-RED flows subscribe to MQTT topics for processing and can republish or store data. A time-series database (historian) retains the data, which Grafana uses for dashboards. All services run in containers on an edge node. A unified web portal provides access to Node-RED, Grafana, and container management (via embedded iframes or integrated UI). The cloud management service allows remote configuration and deployment updates to the edge.
Connectivity and Protocol Conversion Layer
At the bottom layer, the platform interfaces with various industrial protocols to gather data from PLCs, sensors, and machines:
•	OPC UA Connector: Bridges data from OPC UA servers into the MQTT-based data pipeline. OPC UA (OLE for Process Control Unified Architecture) is a common machine-to-machine communication protocol in industrial automation. The connector (which could be a Node-RED flow or a dedicated service) will act as an OPC UA client to subscribe to or poll for specific node values (tags) on the PLC or device. The data is then published to MQTT topics. Node-RED is well-suited for this integration, as it has nodes for OPC UA and can easily route OPC UA data to MQTTemqx.comflowfuse.com. By converting OPC UA data into MQTT messages, the system makes legacy machine data accessible to modern IoT apps and cloud systemsflowfuse.comflowfuse.com.
•	Siemens S7 Connector: Interfaces with Siemens S7 PLCs (which use the S7 protocol over TCP/IP). The connector will read and write PLC data (DB blocks, inputs/outputs, etc.) and publish these as MQTT topics. Tools like the node-red-contrib-s7 node can be utilized within Node-RED to communicate with S7 PLCsflowfuse.com. Node-RED provides a user-friendly way to connect S7 PLCs without deep expertise, by using a drag-and-drop flow to read/write PLC data via the S7 protocolflowfuse.com. This connector service will periodically poll PLC registers or subscribe to change events (depending on PLC capabilities) and translate them to MQTT messages. Similarly, incoming MQTT commands could be written back to PLC addresses if control is needed.
•	Modbus Connector: Interfaces with devices using Modbus (TCP or RTU). This service will poll registers or coils from Modbus slaves at configured intervals and publish values to MQTT. Node-RED offers Modbus nodes as well, which can implement this polling logic. Modbus being a simpler protocol, the connector container or flow will handle connecting to multiple Modbus devices and mapping their registers to MQTT topic namespace.
•	Other Protocols: The architecture can be extended with additional connectors for protocols like OPC DA, EtherCAT, PROFINET, BACnet, etc., by deploying new microservices or Node-RED nodes. The system's design allows plugging in new protocol converters as needed (each likely as a separate container or flow, adhering to a common pattern). Initially, focus is on S7, OPC UA, and Modbus as requested.
Each connector service runs in a container and is configured with the device's network address, protocol parameters, and the mapping of data points (tags) to MQTT topics. In early stages, configuration may be manual (via config files or the UI), but later a network discovery feature can be added to auto-scan for devices and auto-populate accessible tags. (For example, the system could scan a subnet for OPC UA servers or S7 PLCs, then browse their available data points). This discovery mechanism is earmarked for later development as it adds complexity.
Data Tag Discovery: Even without full network scanning, the platform can assist in tag discovery by browsing protocols. For OPC UA, the connector can browse the address space of the server to list available nodes. For S7, if symbolic access is enabled, it can read a list of DBs and addresses. These discovered tags could then be presented to the user in the UI for selection, rather than requiring manual entry of tag names. Such functionality will significantly ease system setup, although it might be implemented in stages.
All connectors publish data to the central MQTT broker using a structured topic hierarchy (for example: factory1/line2/PLC1/temperature etc.), forming a unified namespace for all device data. This unified namespace (a concept often used in IIoT architectures) allows any subscribing component to easily discover and use incoming data by topic.
MQTT Broker (EMQX) for Data Transport
At the core of the platform is the MQTT broker, which functions as the message bus for IoT data. We will use EMQX, a high-performance open-source MQTT broker. EMQX is chosen due to its proven scalability and reliability, capable of handling extremely large numbers of connections and messages with minimal latencygrafana.com. It implements the MQTT protocol (v3.1.1 and v5 support), which is a lightweight pub/sub messaging protocol ideal for IoT scenarios (optimized for low bandwidth and high latency environments)emqx.com.
Role of EMQX: All data from devices, after being converted by the protocol connectors, is published to EMQX on specific topics. Any interested component (like Node-RED flows, analytics services, or external subscribers) can subscribe to those topics to get real-time updates. EMQX ensures decoupling between data producers and consumers – devices send data to the broker without knowledge of who will use it, and consumers can come and go or scale independently.
Key advantages of using EMQX include:
•	High Throughput and Connections: EMQX can handle on the order of millions of messages per second and up to 100 million concurrent device connections in cluster deploymentsgrafana.com. This gives headroom for scaling up the number of devices or sites in the future without changing the broker.
•	Clustering: EMQX supports clustering of multiple broker instances for load balancing and high availability. In our on-premise setup, we might start with a single instance, but for larger deployments, multiple EMQX containers could form a local cluster or a hierarchical setup (e.g., edge EMQX forwarding to a cloud EMQX).
•	Persistent Sessions & QoS: MQTT features like QoS (Quality of Service) levels and retained messages will be utilized for reliable delivery. For instance, critical sensor data could be sent with QoS=1 (at least once delivery) to ensure no data loss over unreliable networks. EMQX handles these aspects and can persist session data for devices so that intermittent connectivity doesn't lose messages.
•	Rule Engine and Extensions: EMQX includes a rule engine (especially in enterprise editions) that can filter and transform messages or directly forward them to databases, HTTP endpoints, etc.grafana.com. In our architecture, we primarily plan to use Node-RED for processing, but EMQX's rule engine could be an alternative path for certain integrations (for example, writing directly to the historian DB from the broker, or simple threshold alerts).
•	Security: EMQX supports TLS encryption, authentication (user/password, certificates, JWT), and ACLs. Our platform will enforce security by requiring device connectors and other clients to authenticate with the broker. We can define access control such that connectors can only publish to their topics, and subscribers (like Node-RED or Grafana via MQTT datasource, if used) have read-only access. For on-prem deployment, internal connections might run unencrypted for performance, but external access (if any) will go through secure channels.
Using MQTT decouples producers and consumers, enabling a flexible architecture. For example, multiple processing services can subscribe to the same sensor data topic without modifying the device connections. This publish-subscribe bus model is a hallmark of modern IoT systems and the unified data layer approach, as seen in many IIoT platforms and the Unified Namespace concept.
EMQX Edge vs Core: Notably, EMQX also offers an "Edge" edition and a product called EMQX Neuron (industrial IoT connectivity gateway)emqx.com. In our design, we've essentially combined those concepts: using EMQX as the MQTT broker and Node-RED (or similar) as the connectivity gateway. EMQX Neuron could be considered in future if we wanted a specialized off-the-shelf connector solution for protocols, but Node-RED and our custom connectors give us more flexibility initially.
Edge Data Processing and Orchestration (Node-RED Flows)
For processing and orchestrating data on the edge, the platform integrates Node-RED. Node-RED is an open-source flow-based programming tool that provides a browser-based visual editor to wire together data flows. We will use Node-RED as a built-in Flow Engine for the platform, allowing users (or developers) to implement custom logic, data transformations, alerting, and integration workflows by simply connecting nodes.
Why Node-RED: Node-RED offers an intuitive interface for building IoT applications without heavy coding – users drag and drop nodes (representing inputs, outputs, and processors) onto a canvas and connect them, forming a "flow". This is ideal for industrial settings where on-site engineers can tweak data handling or create simple automation rules without writing a full program. As the FlowFuse team notes, Node-RED's "intuitive flow-based interface enables creating custom workflows and dashboards without deep technical expertise"flowfuse.com. It supports many protocols and can integrate with web services, databases, etc., making it a Swiss-army knife for IoT data handlingflowfuse.com.
Node-RED Integration in Our Platform:
•	Node-RED will run as a container (possibly a customized Node-RED instance pre-loaded with relevant nodes for our platform – e.g., nodes for MQTT, database, S7, OPC UA, etc.). Litmus Edge uses a customized Node-RED for their flowsdocs.litmus.io, and we can adopt a similar approach by including Node-RED and necessary nodes out-of-the-box.
•	The Node-RED runtime will connect to the EMQX broker using an MQTT input node (to subscribe to topics of interest) and MQTT output nodes (to publish processed data or control commands). This allows Node-RED to ingest data from any device connector in real time.
•	Within Node-RED flows, we can perform various tasks: filtering data, combining streams from multiple sensors, implementing simple alert rules (e.g., if temperature > X then publish an alert topic or send email), formatting data (converting raw values into JSON structures or engineering units), etc. This provides a flexible “logic layer” on the edge.
•	Node-RED can also write to the historian database. There are existing contrib nodes for InfluxDB, PostgreSQL/Timescale, etc., or we can use Node-RED’s HTTP or database nodes to interface with the chosen time-series DB. For example, a flow might take an MQTT message and insert it into InfluxDB for storage.
•	In addition to automated flows, Node-RED can be used for integration with external systems. For instance, if we want to forward certain data to an enterprise system or call an API (like sending a notification or updating a cloud service), Node-RED nodes can do that. This complements the built-in rule engine of EMQX by offering a more user-friendly, code-flexible environment.
Customized Nodes and Functions: We will ensure Node-RED has nodes installed for all necessary functionalities:
- MQTT (built-in) – to subscribe/publish to EMQX.
- OPC UA (node-red-contrib-opcua) – possibly used by connectors or flows directly to query OPC UA servers.
- S7 (node-red-contrib-s7) – if we allow flows to directly poll PLCs, though this might be handled by separate connector containers. In early implementations, we could even have Node-RED do the device polling (one could implement S7/OPC/Modbus polling flows directly), but a cleaner separation is to have dedicated connectors. Still, having these nodes available is good for flexibility.
- Database nodes – e.g., node-red-contrib-influxdb for writing to InfluxDB, or an HTTP request node if using a REST API of a historian, etc.
- Analytics nodes – We might include nodes for simple analytics (like smoothing, anomaly detection, etc.), or integrate with Python via exec function if needed for heavier processing. (Note: Litmus Edge has a separate “Analytics” module with pre-built functions like ARIMA, anomaly detectiondocs.litmus.io, which could be a future extension for our platform. Initially, users can implement custom logic in Node-RED or we consider using libraries).
- Output/Notification nodes – Email or SMS (Twilio) nodes for alerting, etc., so Node-RED flows can send notifications on certain conditions.
Flow Deployment and Management: The platform’s UI will embed the Node-RED editor, allowing authorized users to create and edit flows in real time through their browser (the Node-RED editor will be accessible via the unified web interface, possibly within an iframe or integrated using the Node-RED embedded API). We will need to handle multi-user access and authentication for Node-RED editor, likely by using Node-RED’s built-in adminAuth to require login, or by proxying it behind our platform’s own auth.
We also consider source control for flows and deployment pipeline (especially if multiple edge nodes or dev/staging environments exist). In future, an approach similar to FlowFuse could be used, where flows can be pushed remotely. For now, flows are stored locally on the edge node (in the Node-RED user directory or a mounted volume).
Performance considerations: Node-RED runs on Node.js and can handle many flows, but very complex or high-throughput tasks might demand optimization. If needed, critical data parsing could be offloaded to specialized microservices (like a Python processing service). The EMQX blog example used a Python program to do heavy lifting and store to DBemqx.com. In our design, we might leverage Node-RED for most tasks, but we keep the option open to add dedicated processors (in containers) subscribing to MQTT for, say, computationally heavy analytics or machine learning.
Data Historian (Time-Series Database)
A central requirement is to log time-series data (sensor readings, device tag values, etc.) over time – this is the role of the historian. Instead of a traditional industrial historian (which can be proprietary), we will use an open-source Time-Series Database (TSDB) optimized for IoT data. Possible choices include InfluxDB, TimescaleDB (PostgreSQL), QuestDB, or CrateDB, among others. These databases are designed to efficiently handle large volumes of time-stamped data and allow querying by time ranges, aggregation, and retention policies.
Recommended Approach: For an open-source stack, InfluxDB is a strong candidate due to its widespread use in the IoT community and integration with Grafana. InfluxDB is a high-performance time-series database optimized for fast writes and queries on timestamp-indexed datablog.balena.io. It can handle large volumes of metrics and events, and supports features like data retention policies (to automatically purge or downsample old data) and compression. Alternatively, the project might consider QuestDB, which is also optimized for real-time time-series and showed good performance in recent benchmarksemqx.com. Both are suited to run in a container on the edge.
Key considerations for the historian:
•	Data Schema: We need to define how data will be organized in the TSDB. Typically, for InfluxDB, data is organized into measurements, tags, and fields. For example, a measurement could be "sensor_data", with tags like device=PLC1, signal=temperature and the value as a field. This schema flexibility allows storing heterogeneous signals in one measurement or separate measurements per device type. We will design a schema that mirrors the MQTT topic structure, enabling easy mapping from MQTT messages to DB writes.
•	Data Ingestion: Node-RED flows will handle writing data to the TSDB. For instance, an MQTT message on topic factory1/line1/PLC1/temperature with value 85 °C could be translated by Node-RED into a write to InfluxDB (measurement=factory1_line1 values: temperature=85). We could also use EMQX’s integration (via its rule engine) to directly write to InfluxDB on certain topics as an alternative pathgrafana.com, but using Node-RED gives more control over processing/formatting before storage.
•	Historian Performance: The database must sustain high write throughput since industrial systems can generate many readings per second (imagine dozens of devices each with many tags at 1 Hz or faster). InfluxDB in its 2.x version or QuestDB are built for this, and can handle thousands of writes per second on modest hardware. The edge node should have adequate disk I/O and memory to support the database. We will configure retention so that the disk usage is bounded (for example, raw data might be kept for X days, and aggregated data for longer).
•	Querying: Grafana will query this database for visualization (more on that below). Other components could also query it (for example, an AI module might query historical data for training models). The TSDB should support efficient time-bound queries and aggregations. TimescaleDB (if using Postgres) leverages SQL and might integrate with analytics easier; Influx has its InfluxQL or Flux query languages. We need to ensure whichever is chosen integrates smoothly with Grafana (which supports many TSDBs via plugins).
•	Scaling and Distribution: For the initial on-prem deployment, one instance of the TSDB on the edge node suffices. If needed, a future enhancement could use a clustered time-series database or replicate data to a cloud database for aggregation. For example, a corporate data lake might aggregate data from many edge historians periodically.
The best approach for the historian will balance performance, reliability, and simplicity:
•	InfluxDB – easy to deploy, great Grafana support, built-in retention, but memory usage can be high under load.
•	QuestDB – very high ingestion performance in benchmarksemqx.com, uses SQL, but newer to the community.
•	TimescaleDB – built on PostgreSQL, brings reliability and ACID compliance, and standard SQL (could be good if we want complex queries or to join with other data).
•	We might prototype with InfluxDB (given the existence of the popular MING stack – MQTT, Influx, Node-RED, Grafana – which demonstrates these components working togetherblog.balena.ioblog.balena.io). InfluxDB's time-series engine is optimized for exactly this kind of data (time-indexed sensor readings)blog.balena.io.
Additionally, we will design how to handle historian backup and sync. Since the system is on-premise, data stays locally (which is often desired for data ownership and latency reasons). But an option to periodically export or sync historical data to a central cloud or data warehouse could be provided (for long-term archival or machine learning on aggregated data). This could be done via a cloud connector service or even using the cloud management interface to pull data.
Visualization and Dashboard (Grafana Integration)
For real-time visualization and dashboards, Grafana will be deployed as part of the platform. Grafana is an open-source analytics and monitoring UI that can connect to the time-series database and render interactive charts, graphs, and alertsemqx.com. It is widely used for IoT and industrial monitoring due to its flexibility and rich plugin ecosystem.
Grafana’s Role:
•	Grafana will connect to the historian database (e.g., via the InfluxDB or PostgreSQL data source plugin) and provide dashboards for the end-users to monitor device data. Users can see trends over time, current values, and historical comparisons. Grafana supports arranging multiple visualizations on a single dashboard with various chart types (time-series line graphs, gauges, bar charts, etc.).
•	We will create some default dashboards for common use-cases – for example, an overview showing key metrics from all connected devices (temperatures, pressures, speeds, etc.), or more detailed per-machine dashboards. These can serve as starting templates that users can clone and customize.
•	Grafana also supports alerting: for instance, if a metric goes out of a predefined range, Grafana can trigger an alert (via email, Slack, etc.). This complements Node-RED’s ability to send alerts. Grafana’s alerting might be used for conditions that are purely based on numeric thresholds in the historian data.
•	Grafana’s user management will be integrated with our platform’s auth if possible, or we might keep a separate admin user for Grafana whose interface is embedded. Ideally, the unified portal handles authentication and single sign-on to Grafana (to avoid multiple logins). Grafana supports OAuth and also anonymous viewing for certain dashboards – we might allow read-only viewing of dashboards without requiring a login, if security policy permits.
The platform’s web interface will likely embed Grafana dashboards via iframes. Grafana has an option to share dashboards as embedded iframes (which can be done by enabling anonymous access or using a snapshot/embed link). In the early stage, a simple approach is to host Grafana at http://edge:3000 and in our React app, embed an <iframe src="http://edge:3000/d/..."> for the desired dashboard. The React app can provide navigation to different embedded Grafana dashboards as needed. Later, a tighter integration could involve using Grafana's HTTP API to dynamically get data or manage dashboards.
By using Grafana, we leverage a proven visualization tool rather than building custom charts from scratch. As highlighted by an IoT case study, Grafana provides intuitive real-time dashboards so stakeholders can continuously monitor device conditions, observe trends, and track performance metricsemqx.com. The combination of a time-series DB + Grafana for visualization is a common and effective pattern in IoT solutionsemqx.com.
Container Orchestration and Management (Docker/Kubernetes)
All components (connectors, EMQX broker, Node-RED, database, Grafana, etc.) will run as Docker containers. To manage these containers cohesively on the edge device, and to allow scaling and dynamic management, we will introduce a container orchestration layer. We have two primary choices:
•	Docker Compose / Docker Engine: A simpler approach where the edge runs a Docker engine and we use Docker Compose (or Docker APIs) to manage the multi-container application.
•	Kubernetes (K3s): A more robust approach using a lightweight Kubernetes distribution like K3s to orchestrate containers (pods) on the edge node. K3s is specifically designed for resource-constrained environments and IoT use-casesk3s-io.github.io.
Recommended: Use K3s (Lightweight Kubernetes) on the edge node. K3s is a fully conformant Kubernetes that has a small footprint (<100 MB binary) and is optimized for running on a single node or small cluster at the edgek3s-io.github.io. This gives us the power of Kubernetes – such as declarative deployments, easy scaling, self-healing, and a robust API for management – without the overhead of a full data-center Kubernetes setup. With K3s, each edge node can effectively run its own Kubernetes cluster (single-node cluster, or possibly a small multi-node cluster if the site has multiple gateways).
Using Kubernetes provides the following benefits for our platform:
•	Declarative Deployment: We can define each microservice (EMQX, Node-RED, etc.) as a Kubernetes Deployment (or StatefulSet for the database). The system can be described in YAML manifests or Helm charts, making it easier to reproduce and update.
•	Auto-Restart & Self-Healing: If a container crashes, Kubernetes will automatically restart it, improving reliability for an always-on industrial system.
•	Scaling: We can scale components by adjusting the replica count. For example, if the load on Node-RED grows, we could spin up multiple instances behind a load balancer, or similarly add more replicas of connectors if needed (this might not be needed initially, but Kubernetes makes it possible).
•	Networking & Service Discovery: Kubernetes provides an internal network (Service mesh) where containers can find each other by service name (e.g., emqx:1883 for the broker). This simplifies configuration of endpoints.
•	Container Updates/Rollback: With proper configuration, we can update components with minimal downtime using rolling updates, and roll back if something fails.
•	Cloud Integration: Kubernetes at the edge can be linked with cloud management systems. For instance, a central cloud service could push Kubernetes manifest updates to the edge clusters (using an agent or something like Rancher or ArgoCD). This aligns with our cloud-configurable requirement.
If not Kubernetes, an alternative is to manage via Docker APIs. In fact, Litmus Edge appears to manage Docker containers directly via its Edge Manager (they mention using Docker Engine APIs and Compose definitions in their marketplace)litmus.io. We can achieve similar results with Docker, but Kubernetes is increasingly the standard even at the edge, and it gives us the future-proof scaling.
Embedded Container Management App: One of the key features requested is a UI to manage these containers/pods. This means the user should be able to:
•	See which services (containers) are running, their status, resource usage, etc.
•	Start, stop, or restart services.
•	Deploy new services (for example, if the user wants to add a new custom microservice or connector).
•	Possibly upload container images or choose from a catalog (like how Litmus Edge has a marketplace of applicationslitmus.iolitmus.io).
Instead of building everything from scratch, we can take inspiration from Portainer, which is an existing web UI for managing Docker/Kubernetes environments. Portainer's approach is to provide a central UI to control container lifecycle across many edge devicesportainer.ioportainer.io. In our case, we want this capability embedded into our platform UI (so users don't have to use a separate tool).
Implementing Container Management:
•	If using Kubernetes: We could either embed the Kubernetes Dashboard (the official web UI) or utilize the Kubernetes API to query and modify resources from our React app. For simplicity, an existing solution like Portainer can be leveraged. Portainer has an API and web component that might be iframed or integrated. However, Portainer has its own look-and-feel; building a custom UI in React that calls Kubernetes API (via a proxy) might provide a more unified experience.
•	If using Docker directly: We could integrate with the Docker Engine API (which Litmus seems to do for their container UIlitmus.io). The React app could call a backend service that wraps Docker API calls to list containers, start/stop them, etc., or again integrate Portainer CE which has an API/UI for this.
Given this is a "very ambitious project", it might be acceptable to start by bundling an existing UI like Portainer in an iframe, then gradually replace with a custom React components. Portainer Business edition even has an "Edge" feature for managing edge devices with an agentportainer.io, but that might be overkill if we plan to implement our own cloud manager.
We will plan for a Containers page in the React app where users can:
•	View a list of available containerized applications (e.g., EMQX, Node-RED, InfluxDB, Grafana, plus any user-deployed ones).
•	Deploy a new application: This could present a form to input an image name and settings (port, env variables, volumes) and then our backend or Kubernetes will launch it. (Litmus Edge's marketplace UI likely does something similar with a wizard for multiple container definitionslitmus.io).
•	Manage lifecycle: Buttons to restart/stop a container, or update it (pull a new image version).
Auto-Spinning New Pods: The user specifically mentioned "(auto) spin new pods when needed." This suggests auto-scaling or dynamic deployment. With Kubernetes, we can set up Horizontal Pod Autoscalers for certain services if appropriate (e.g., if CPU usage of Node-RED gets high, auto-create a second Node-RED pod). However, autoscaling at the edge might be limited by hardware. Alternatively, "auto spin up new pods" could refer to automatically deploying new connector pods when new devices are discovered (e.g., detect a new PLC and automatically start a connector for it). That kind of dynamic behavior would require a smart agent or the cloud manager instructing the edge to deploy something new upon discovery. In early stages, it might simply be manual (user triggers deployment via UI). But we keep in mind the architecture should allow automation – e.g., the Node-RED flow itself could request the orchestrator to launch a new service if needed (with proper permissions).
Resource Constraints: Running Kubernetes and multiple containers on an edge device (like an industrial PC or IoT gateway) requires sufficient resources. We should specify that the edge node should have a multi-core CPU, a few GB of RAM at least (depending on how many services). K3s has a small footprint, but EMQX, Node-RED, and InfluxDB all consume memory. For example, a moderate deployment might need 4GB RAM and 32GB disk as a baseline. The design should allow running on x86_64 or ARM (if using something like a Raspberry Pi or NVIDIA Jetson, etc., all chosen components have arm64 images available).
In summary, container orchestration via Kubernetes will give our platform enterprise-grade deployment management on the edge. The unified web UI will abstract this, so the user sees a friendly interface instead of having to run kubectl commands. As Portainer’s CEO noted, the challenge is managing containerized apps across many distributed IoT devices, and a centralized intuitive UI makes this simple at scaleportainer.io. Our platform seeks to incorporate that principle directly into the product.
Unified Frontend Web Application (React-based)
On top of all the backend microservices, we will develop a unified frontend web application (using React with Vite for fast development) that ties everything together for the end user. This web UI is essentially the "operation center" for the platform, giving users access to all features through their browser.
Features of the Web UI:
•	Dashboard/Home: An overview page showing system status (e.g., broker connections count, system health, summary of data flows). It might display high-level stats or a summary Grafana panel for key metrics.
•	Devices/Connectors Management: A section where users can see all configured devices and their connectors. They should be able to add a new device (which involves deploying or configuring a connector for it) here. For example, "Add Device" could allow selecting type (OPC UA, S7, etc.), entering connection details (IP, creds, etc.), and then the system will either configure Node-RED flow or deploy a container accordingly. Initially, this might just edit a config file that connectors read, but ideally it triggers the orchestrator to start a new connector instance if needed.
•	Data Flows (Node-RED): This will embed the Node-RED editor UI. Likely an iframe pointing to the Node-RED editor (running on, say, http://edge:1880). We will customize Node-RED to auto-login or allow access via our UI (to avoid credential issues, maybe generating a session token). Users can switch to this tab to modify flows. We might also create some pre-built flows (for common tasks) accessible via templates.
•	Historian/Analytics: We might have a page showing data storage status (like how many data points, last entries, etc.). If the historian supports SQL or queries, a simple query interface could be provided (though this is more of a power-user feature; typical users will rely on Grafana for data viewing).
•	Visualization (Grafana): The UI will provide one or more pages for Grafana dashboards. We can either embed specific dashboards or even fully embed Grafana (hiding its menu to make it look native). Another approach is to use Grafana's HTTP API to fetch chart images or data and then render in our own custom UI components – but that is complex and duplicates Grafana functionality. So initially, embedding via iframe is pragmatic. We will ensure it's done in a secure way (maybe using Grafana in anonymous mode or handling auth tokens).
•	Container Management: A dedicated section (perhaps labeled "Apps" or "Services") where the user manages the containers/pods on the system. This can use a library or our own implementation to list all running containers, show their status (running/stopped), logs (maybe link to logs), and allow basic actions. We can integrate a terminal for advanced debugging if needed (or instruct users to SSH for that).
o	If we integrate an existing solution like Portainer, this section might literally just embed Portainer's UI. But to avoid confusion, a custom UI is nicer. The UI can call our backend APIs to do things like kubectl get pods or Docker API calls.
o	Over time, this might evolve into a marketplace concept like Litmus, where the user can choose to deploy standard apps (like “Grafana” is one, “Node-RED” is one, etc., which are already present by default, and maybe additional ones like “Python Notebook” or “TensorFlow Serving” if they want to add AI models, etc.). Each app could be defined by a compose snippet or Helm chart behind the scenes.
o	Security: We will restrict this management to admin users because it effectively allows executing arbitrary containers which could compromise the system if misused. RBAC controls might be necessary if multiple roles exist (like operator vs admin).
•	User Management & Authentication: The platform UI will handle user login, likely with different roles (Admin, Engineer, Viewer, etc.). Admin can manage everything, Engineers can edit flows and devices, Viewer can only view dashboards. This requires an identity management component – possibly a simple local user database or integration with an external IdP for enterprise use. We may not implement full IAM in the first version, but it's good to plan for it.
•	Settings: A section for system settings (network config, cloud connection settings, etc.).
React (Vite): We choose React for its ecosystem and flexibility in building dynamic UIs. Vite will give us a fast dev environment and modern JS bundling. The UI will likely interact with various backend APIs:
•	Some provided by our own control backend (possibly a small Node.js or Go service running alongside to handle orchestrator API calls, authentication, etc.).
•	Some direct from components: e.g., Grafana has its own HTTP API for dashboards; Node-RED has an admin API (but we mainly use its GUI).
•	If using Kubernetes, we might run the Kubernetes API server in readonly/proxy mode accessible or run kubectl commands from a backend service. Alternatively, K3s’s kube-api can be accessed via a service account token to perform actions.
•	For EMQX monitoring, we can use EMQX’s REST API or MQTT stats topic to show broker status (or use Grafana which can display EMQX metrics via a dashboardgrafana.com).
One challenge is integrating multiple web apps (Grafana, Node-RED, Portainer) into one interface. Using iframes is straightforward but sometimes clunky (each has its own toolbar/UI). We will aim to at least skin them similarly or hide redundant navbars if possible:
•	Grafana in kiosk mode (no top bar).
•	Node-RED can maybe be launched in an editor-only mode.
•	Alternatively, for Node-RED, since it's so central, we might embed Node-RED within our React app via a custom integration. There is the possibility to use Node-RED as a library or use its runtime API, but likely not worth the reinvention. We can keep it separate but well-integrated visually.
Edge-Cloud Integration
While the system is edge-centric, the design includes a path for cloud-based management and integration:
•	Cloud Configuration Portal: A cloud-hosted service (could be a multi-tenant web app or just an instance per company) where all edge nodes register themselves. This is analogous to Litmus Edge Manager or Portainer's central systemlitmus.ioportainer.io. Through this portal, an admin could oversee all deployments (multiple sites), push updates, or retrieve data.
•	Each edge node would run an agent that maintains a connection to the cloud. This could be done via MQTT (edges subscribe to a cloud MQTT broker for commands), or via a webSocket/HTTP long-poll to the cloud service. For example, when an admin on cloud says "update Node-RED flows on Edge-1", the cloud service would send a message to that edge's agent, which then applies the change (perhaps by pulling new flow definitions or deploying a container update).
•	Configuration Sync: Cloud should allow defining the desired state (e.g., what containers and versions should run, what flows, etc.) and the edge should sync to that. Tools like GitOps (ArgoCD) could be used: edges could point to a git repo for their K8s manifests. However, for simplicity, a direct publish/subscribe of configuration can be done. EMQX itself could be used as the backbone of config distribution (each edge could have an MQTT client connected to cloud, listening on a topic like config/edge1/# for instructions).
•	Remote Monitoring: The cloud portal could show high-level metrics from each edge (like are containers healthy, resource usage, last data received, etc.). For this, each edge can periodically report status to the cloud (either via MQTT or an API call).
•	Cloud Data Integration: If desired, select data streams can be forwarded to cloud analytics platforms. For instance, critical metrics could be simultaneously published to a cloud MQTT broker or to an IoT cloud service (AWS IoT, Azure IoT Hub, etc.). Our platform might integrate this via an optional connector or via EMQX bridging (EMQX can bridge messages to another MQTT broker).
•	In Phase 1, the cloud integration might be minimal – perhaps just the ability to remotely access the web UI via a secure tunnel or manage a few configs. But designing the system with cloud in mind ensures that scaling out to many sites is feasible. We should keep configuration as code or in a portable form (so that one can replicate it centrally).
Security for cloud-edge communication is paramount: all traffic should be encrypted (TLS), and edges must authenticate with the cloud. Perhaps each edge has an RSA key or token provisioned during installation.
On-Prem vs Cloud: Some clients may choose to run the "cloud" part on their own data center (private cloud) to keep everything in-house. We can allow the management server to be self-hostable. Or optionally, we provide a SaaS for it. This is more a business decision, but technically our architecture remains similar – a central controller and multiple edges.
Security and Reliability Considerations
Industrial systems require high reliability and must not compromise safety or operations. Some considerations:
•	Isolation: By containerizing each component, we isolate them from each other to some degree. If one service fails or is compromised, others continue running. We will use network segmentation (via Docker/K8s networks) so that, for example, device connectors only talk to the broker and not directly to other containers unless needed.
•	Access Control: Within the platform, ensure that only authorized users can change configurations or flows. Use strong credentials for services like EMQX, Node-RED, Grafana (disable default admin/admin stuff). Possibly integrate an SSO across the components.
•	TLS Everywhere: Use secure protocols where possible. MQTT can be run over TLS (EMQX supports it on port 8883) so that if any external device connects, it's secure. Inside the local network, one might run plain MQTT for performance, but it's better to encrypt even on-prem communications if the network is not fully trusted.
•	Data Backup: The historian database should be backed up or at least the important data should be exportable. We might include a periodic backup job (e.g., daily dump of InfluxDB or continuous write to a second DB).
•	Failover and Redundancy: In a single node deployment, if that node fails, the system goes down. For critical uses, we could support an HA cluster at the edge: e.g., two edge devices running the platform in parallel, perhaps with one as hot standby. Kubernetes could manage this if we had multiple nodes, but that adds complexity. Another approach is using a robust hardware with redundant power, etc. This depends on requirements; at PhD-level design, we'd mention that for production, redundancy should be considered for the broker and database (like EMQX could cluster across two nodes, and a replicated DB could be used).
•	Real-time constraints: MQTT and Node-RED are not truly real-time (in deterministic sense), but are generally fast (latency ~ milliseconds). For most monitoring that is fine. If certain control functions require real-time behavior, the system should probably not directly close the loop via this platform (use PLC logic for that). We note that this platform is mainly for monitoring, higher-level control and analytics, not for ultra-fast control loops.
•	Edge Autonomy: Ensure the edge can run offline if the cloud connection is lost. All critical functions (data collection, local visualization, local storage) should continue even without internet. The cloud is just for convenience in config and aggregate analysis, not a dependency for operation (unless explicitly desired for some reason).
Future Extensions and Roadmap
The architecture described is modular and can accommodate future enhancements. Some likely future extensions include:
•	Automated Network/Device Discovery: As noted, adding a discovery service that scans the network for known device signatures (open ports for S7, OPC UA, etc.) would streamline onboarding. This could be a container using nmap or device-specific discovery protocols (like PROFINET discovery for Siemens, or mDNS/LLDP sniffing). Once devices are found, it could alert the user or auto-add them in a pending list for configuration.
•	AI/ML and Advanced Analytics: Future versions might integrate AI for predictive maintenance, anomaly detection, or optimization. This could be done by:
o	Training models on the historical data (maybe in the cloud or offline), then deploying the model to the edge. For example, a TensorFlow or PyTorch model could be packaged in a container and subscribed to relevant MQTT topics to score incoming data. If an anomaly is detected, it can publish an alert.
o	Using built-in analytics engines. Litmus Edge has an Analytics module with common functions (ANOVA, ARIMA, etc.)docs.litmus.io. We could implement similar functions either via Node-RED nodes (there are some Node-RED nodes for ML) or via a separate "Analytics" microservice that the user can configure with a pipeline of operations (low-code but more focused on math).
o	Given the complexity, we assume AI features will be added once the core platform (data collection, storage, UI) is stable. This is in line with the request "AI is for later versions".
•	Edge-to-Edge and Federated Capabilities: Possibly linking multiple edge nodes in a hierarchical fashion. E.g., edges sharing data with each other for global optimization on-prem, or a cluster of edges balancing workloads (one could imagine if one edge is overloaded, some connectors move to another nearby edge). This enters advanced territory and likely not needed initially, but a Kubernetes base makes it conceivable to scale across nodes.
•	Enterprise Integration: Connecting to existing MES/SCADA or enterprise systems. For example, pushing production data to an MES, or fetching orders from ERP to feed into Node-RED to control production. Node-RED can connect via OPC UA (it can act as an OPC UA server as well to expose dataflowfuse.com), bridging the IT/OT divide. The platform could serve as a unified namespace that higher-level systems subscribe to, hence acting as a gateway.
•	User Interface Enhancements: Over time, the React app can be enriched with more native visualizations (maybe using libraries like Plotly or ECharts for custom quick views) to complement Grafana. Also, more wizards and guides to simplify tasks (e.g., a wizard for connecting a new PLC guiding through steps).
Data Flow and Process Descriptions
To clarify how the pieces work together, let's walk through some typical flows in the system:
1. Device Data Ingestion and Monitoring Flow:
•	A field device (e.g., a temperature sensor connected to a Siemens S7 PLC) updates a value in the PLC memory. The PLC might be programmed to update that value every second.
•	Our S7 Connector (running in a container) is configured to read that particular address from the PLC at 1 second intervals (or subscribe if PLC supports push). When it reads the new value, the connector publishes an MQTT message to EMQX. For example, it publishes to topic factory1/lineA/PLC1/tempSensor1 with payload { value: 85.0, unit:"C" }.
•	EMQX broker receives this message and routes it to any subscribers.
•	The Node-RED Flows service is subscribed (via an MQTT-in node) to factory1/lineA/PLC1/# (all topics for PLC1). The message arrives in Node-RED. A flow might process it – for instance, apply a calibration offset or just pass it through. Node-RED then, in the same flow, writes the data to the historian DB (using an InfluxDB out node, it writes the value 85.0 with the appropriate tags for factory, device, sensor).
•	The Node-RED flow could also implement a high temperature alarm: it checks if value > 90, and if so, publishes to an alert topic or sends an email. In this case 85 is normal, so no alert.
•	The data is now stored in the time-series DB with a timestamp.
•	Grafana is configured with a dashboard that has a graph for "Temperature Sensor 1". It queries the database (e.g., "SELECT value FROM tempSensor1 WHERE time > now()-1h"). Within a second or two, the new data point (85°C) is available in the DB and Grafana's graph on the user's screen updates to show the latest value.
•	The user glancing at the Grafana dashboard (maybe on a big screen in the control room, or via the web portal) sees the real-time trend of that sensor.
•	If the value had exceeded the threshold, either Node-RED would have sent an alert (e.g., as an MQTT message on an alerts topic, or an email), or Grafana's alerting would catch it on the next evaluation cycle and notify as configured.
Throughout this flow, MQTT ensures decoupling. For example, if Node-RED was down for maintenance, EMQX could be configured to retain the last value or simply, the data would queue or be missed but PLC publishing wouldn't be blocked. Once Node-RED is up, data flows again. We might want to implement store-and-forward for critical data if Node-RED or DB is temporarily unavailable (this could be done via EMQX retained messages or a persistent local queue in Node-RED).
2. Container Deployment/Management Flow:
•	Suppose a new requirement comes in: connect a new Modbus sensor or deploy an additional analysis microservice. The user (with admin rights) accesses the Container Management section of the UI.
•	They click "Add Container" (or "Deploy New App"). Our React UI might present options: either select from predefined templates or enter a custom image. If this platform in future has a marketplace, the new app could be something like "Node-RED instance for Lab equipment" or a custom script container.
•	The user chooses "Modbus Connector" from templates. The UI might then ask for Modbus IP and device details. Upon confirmation, the UI sends this info to the backend (either directly to Kubernetes API or via our controller service).
•	The system then launches a new container (pod) running the Modbus Connector service, passing the configuration (device IP, registers to poll, topic prefix) as environment variables or mounting a config file. In Kubernetes, this would be creating a new Deployment and ConfigMap for it.
•	Within seconds, the new connector starts and begins publishing data to EMQX. The user can now see the device data in MQTT (maybe a debug view in UI shows incoming messages) and can proceed to create Grafana panels for it or Node-RED flows to process it.
•	The UI updates the list of running containers to include this new Modbus connector. The user can manage it like others (stop it if needed, etc.).
•	In parallel, if this user had multiple edge nodes and was using the cloud portal, they could choose which edge to deploy to. The cloud manager would then send the deployment instructions to the selected edge.
3. Remote Configuration via Cloud:
•	An admin logs into the Cloud Management Portal and sees all their edge nodes (Edge1, Edge2, etc.). They notice Edge1 is running an older version of the EMQX broker and decide to update it.
•	Through the portal, they select Edge1 and choose "Update EMQX to v5.1". The cloud then sends a command to Edge1's agent (maybe via MQTT message on a control channel) with instructions or a new compose file.
•	The edge agent (which could be integrated with our container manager service) receives this and applies the update. If using Kubernetes, it might pull a new container image for EMQX and perform a rolling update.
•	After update, Edge1 sends confirmation back to cloud. The admin sees that EMQX is now updated. This process should be done carefully to avoid downtime (maybe EMQX had two replicas and updated one by one).
•	Similarly, the admin could remotely change a Node-RED flow by pushing a flow file via the portal, or update a connector configuration (e.g., add a new tag to collect).
These flows illustrate the end-to-end operation of the system under different scenarios.
Implementation Technologies and Stack Summary
To summarize the technology stack and choices:
•	Programming Languages: Mostly leveraging existing tools (EMQX in Erlang, Node-RED in Node.js, Grafana in Go) rather than custom coding for those. The custom parts will be the React front-end (JavaScript/TypeScript) and possibly a backend service for the UI (could be Node.js/Express or a small Go/Python service) to coordinate with Kubernetes and handle auth.
•	Docker Containers: All components will be provided as Docker images (either official ones like emqx/emqx, nodered/node-red, influxdb, grafana, or custom-built ones for connectors and UI).
•	Orchestration: K3s (Kubernetes) on the edge device. Alternatively Docker Compose for a simpler start; but K3s is preferred for production.
•	Host OS: A Linux-based OS on the edge device (could be Ubuntu, Debian, or a container-focused OS like balenaOS if using balena, but that's optional). K3s can run on standard Linux.
•	Hardware: Industrial PC or gateway with at least ARMv7/ARM64 or x86_64 architecture. Requirements scale with usage, but e.g., an 8-core CPU, 8GB RAM, 128GB SSD would comfortably run a moderate deployment (this is not strict - can be less if fewer components).
•	Networking: The platform should be deployed in the OT network of the factory (to reach PLCs). It might have dual network interfaces if needed (one to OT, one to IT network). All containers share the host network or use a Kubernetes service network; connectors must be able to reach device IPs. Firewalls must allow the necessary ports (MQTT default 1883/8883, Node-RED 1880, etc., though those might not be exposed beyond localhost or VPN).
•	Development: We will use modern dev practices - IaC (Infrastructure as Code) for deployment definitions, CI/CD for building container images, etc. Testing will include simulation of devices (perhaps using simulators for S7 and OPC UA to verify data flows).
•	Documentation: Provide documentation for end users on how to add devices, create flows, dashboards, etc. Possibly include some mermaid diagrams (like above) in docs to explain flows.
In conclusion, this proposed architecture is comprehensive and modular, aligning with state-of-the-art Industry 4.0 edge platforms. It leverages proven open-source components (EMQX, Node-RED, InfluxDB, Grafana, Kubernetes) to achieve the functionality of a custom Litmus-like edge system. The design emphasizes scalability, flexibility (low-code customization via Node-RED), and integration capability, while ensuring the system can be managed easily through a unified web interface. As the platform evolves with advanced features like AI and automatic discovery, its core framework will support these enhancements thanks to the solid foundation laid out in this specification.
Sources:
•	EMQX IoT Stack Example – Node-RED for OPC UA, EMQX for MQTT, QuestDB for storage, Grafana for visualizationemqx.comemqx.com.
•	EMQX Broker Performance – supports 100M+ IoT connections with 1ms latency in clustersgrafana.com.
•	Node-RED Integration – provides low-code visual flows to connect industrial protocols (OPC UA, S7) to IoT systemsemqx.comflowfuse.com.
•	InfluxDB (Time-Series DB) – optimized for high-volume time-stamped data, ideal for IoT sensor loggingblog.balena.io.
•	Grafana – open-source visualization for real-time monitoring of IoT data via flexible dashboardsemqx.com.
•	Litmus Edge Reference – similar platform using Node-RED flows, marketplace for container apps, and cloud managementlitmus.iolitmus.io.
•	Portainer (Edge Container Management) – example of centralized UI to deploy/manage containers across IoT devices, highlighting simplicity at scaleportainer.io.
•	Node-RED Bridges for IIoT – Node-RED can bridge OPC UA to MQTT, enabling unified communication between legacy industrial devices and modern appsflo

🚀 Industrial IT-OT Platform
An open-source, containerized edge platform to bridge industrial PLCs, MQTT messaging, historian storage, and modern UIs for observability, automation, and container control.
Project Goals: Build a modular, production-ready IIoT system that runs on-premise, integrates with cloud for config, supports common industrial protocols (Modbus, OPC UA, S7), real-time data visualization, and extensibility via AI, Node-RED, and embedded management UIs.
________________________________________
📚 Table of Contents
•	🧭 System Architecture
•	📦 Containers Overview
•	⚙️ Setup & Deployment
•	🔁 Data Flow
•	🛠 Configuration & Access
•	🔐 Security Tips
•	👩‍💻 Developer Notes
•	🗺 Roadmap
________________________________________
🧭 System Architecture
flowchart TD
    subgraph Field Devices
        A1[PLC S7] --> B1
        A2[Modbus RTU/TCP] --> B1
        A3[OPC UA Server] --> B1
    end

    B1[Protocol Gateways]
    B1 --> C[MQTT Broker (EMQX)]
    C --> D1[Historian (TimescaleDB)]
    C --> D2[Node-RED]
    C --> D3[AI Engine (Future)]
    C --> E[Realtime Gateway]
    E --> F[React Frontend]
    D2 --> F
    G[Grafana] --> F
    H[Container Manager UI] --> F
________________________________________
📦 Containers Overview
Name	Purpose	Port(s)
emqx	MQTT broker to handle PLC connections & publish data	1883, 8883
node-red	Low-code flow editor for processing & automation	1880
grafana	Visualization dashboards for metrics/historian data	3000
historian	Time-series DB (TimescaleDB or InfluxDB)	5432 / 8086
protocol-gateways	Converts S7, Modbus, OPC-UA to MQTT	Custom
react-frontend	Main unified UI with iframes to all modules	5173
container-ui	Embedded container & K8s control panel	9090
________________________________________
⚙️ Setup & Deployment
🚀 Quickstart with Docker Compose
git clone https://github.com/your-org/iiot-platform.git
cd iiot-platform
docker-compose up -d
🐳 Example docker-compose.yml
version: '3.8'
services:
  emqx:
    image: emqx/emqx:latest
    ports:
      - "1883:1883"
      - "8883:8883"

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"

  node-red:
    image: nodered/node-red:latest
    ports:
      - "1880:1880"

  historian:
    image: timescale/timescaledb:latest-pg14
    environment:
      - POSTGRES_PASSWORD=yourpass
    ports:
      - "5432:5432"

  react-frontend:
    build: ./frontend
    ports:
      - "5173:5173"

  container-ui:
    image: your/container-ui
    ports:
      - "9090:9090"
☸️ Kubernetes (Optional)
•	Helm charts and manifests coming soon.
•	Auto-scaling and rolling update strategies recommended for ingestion services.
________________________________________
🔁 Data Flow
sequenceDiagram
    participant PLC as PLC
    participant Gateway as Protocol Gateway
    participant MQTT as EMQX Broker
    participant DB as Historian
    participant Frontend as React UI

    PLC->>Gateway: Send raw data (e.g. Modbus)
    Gateway->>MQTT: Publish as topic
    MQTT->>DB: Write payload to TimescaleDB
    MQTT->>Frontend: Forward real-time values
    DB-->>Frontend: Historical trend on request
________________________________________
🛠 Configuration & Access
Access URLs
Service	URL
Frontend	http://localhost:5173

EMQX Dashboard	http://localhost:18083

Grafana	http://localhost:3000

Node-RED	http://localhost:1880

Container UI	http://localhost:9090

MQTT Topics
•	plc/{device_id}/state
•	plc/{device_id}/metrics
•	alerts/{zone}
Node-RED Tips
•	Drag MQTT input node, subscribe to topic, parse JSON.
•	Use InfluxDB or PostgreSQL nodes for writing historical data.
________________________________________
🔐 Security Tips
•	Change all default passwords (Postgres, Grafana, EMQX, Node-RED).
•	Use TLS for MQTT (port 8883).
•	Use JWT/OAuth2 for frontend APIs.
•	Limit network access to control services.
________________________________________
👩‍💻 Developer Notes
Frontend (React + Vite)
•	frontend/ contains the iframe-integrated dashboard.
•	Built with TailwindCSS + shadcn/ui.
•	Each iframe loads Grafana, Node-RED, container manager securely.
Adding New Containers
•	Add to docker-compose.yml or K8s manifest.
•	Register port & UI route in React shell.
Contributing
git checkout -b feature/your-module
npm run dev  # or docker-compose up
________________________________________
🗺 Roadmap
•	MQTT broker with protocol conversion
•	Node-RED + Grafana integration
•	Embedded container management UI
•	Automatic tag discovery (OPC UA, S7)
•	AI-based decision engine (predictive/LLM-driven)
•	Cloud config UI + remote deployment hooks
•	Multi-tenant user auth (Keycloak or Auth0)
________________________________________
Made with 💡 for modern factories. Contributions welcome!

